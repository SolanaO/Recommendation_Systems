{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='darkblue'> Recommendations with IBM Watson</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several recommendation methods are investigated on real data from the IBM Watson Studio platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Table of Contents</font>\n",
    "\n",
    "1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "2. [Rank Based Recommendations](#Rank)<br>\n",
    "3. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "4. [Content Based Recommendations](#Content-Recs)<br>\n",
    "5. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "6. [Extras & Concluding](#conclusions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Environment SetUp</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries and packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Recommendation systems library\n",
    "import surprise\n",
    "\n",
    "# Packages and libraries for content based recs\n",
    "import re\n",
    "\n",
    "# NLP packages\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Data processing packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import linear kernel to compute the dot product\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization packages and libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Choose style and color palette\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "colors = sns.color_palette('PuBuGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2 decimal places in output display\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# Don't wrap dataframe across additional lines\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "# Set the maximum widths of columns\n",
    "pd.set_option(\"display.max_colwidth\", 90)\n",
    "\n",
    "# Set max rows displayed in output to 20\n",
    "pd.set_option(\"display.max_rows\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Upload Preprocessed Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>NOTES</b>:\n",
    "    <ul>\n",
    "        <li>There are 4 files to upload.</li>\n",
    "        <li>Two files are centered on the user, the remaining two are centered on the article.</li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the user-item interaction files\n",
    "df = pd.read_csv('data/df.csv', index_col=[0])\n",
    "articles_per_user = pd.read_csv('data/articles_per_user.csv', index_col=[0])\n",
    "\n",
    "# Read the articles information files\n",
    "df_content = pd.read_csv('data/articles_community.csv', index_col=[0])\n",
    "users_per_article = pd.read_csv('data/users_per_article.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>articles_count</th>\n",
       "      <th>unique_articles_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1430, 1430, 732, 1429, 43, 109, 1232, 310, 1293, 1406, 1406, 329, 585, 310, 1305, 105...</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1314, 1305, 1024, 1176, 1422, 1427]</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1429, 1429, 1330, 213, 1172, 1431, 1429, 1059, 1057, 29, 788, 1172, 868, 12, 1429, 10...</td>\n",
       "      <td>82</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1338, 1314, 1330, 1330, 1427, 1160, 1162, 1391, 1162, 887, 1420, 1394, 1305, 1314, 13...</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1276, 1351, 1166, 1351, 1351]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                                                                 article_id  articles_count  unique_articles_count\n",
       "0        1  [1430, 1430, 732, 1429, 43, 109, 1232, 310, 1293, 1406, 1406, 329, 585, 310, 1305, 105...              47                     36\n",
       "1        2                                                       [1314, 1305, 1024, 1176, 1422, 1427]               6                      6\n",
       "2        3  [1429, 1429, 1330, 213, 1172, 1431, 1429, 1059, 1057, 29, 788, 1172, 868, 12, 1429, 10...              82                     40\n",
       "3        4  [1338, 1314, 1330, 1330, 1427, 1160, 1162, 1391, 1162, 887, 1420, 1394, 1305, 1314, 13...              45                     26\n",
       "4        5                                                             [1276, 1351, 1166, 1351, 1351]               5                      3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the user_item dataframes\n",
    "articles_per_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>article_id</th>\n",
       "      <th>views</th>\n",
       "      <th>users_accessed</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...</td>\n",
       "      <td>Detect bad readings in real time using Python and Streaming Analytics.</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>[495, 495, 495, 503, 233, 552, 1347, 1051, 785, 2992, 3216, 3570, 4571, 4836]</td>\n",
       "      <td>detect malfunctioning iot sensors with streaming analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n * kaggle.com\\r\\n\\r\\nCommunicating data s...</td>\n",
       "      <td>See the forest, see the trees. Here lies the challenge in both performing and presenti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Communicating data science: A guide to presenting your work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Paths\\r\\n * Courses * Our Courses\\r\\n    * ...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Big Data.</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>[676, 668, 668, 1145, 23, 23, 60, 60, 665, 98, 668, 794, 217, 60, 1401, 46, 1577, 789,...</td>\n",
       "      <td>this week in data science (april 18, 2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCALE - BOOST THE PERFORMANCE OF YOUR\\r\\nDI...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of scaling persistent storage, but introdu...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>DataLayer Conference: Boost the performance of your distributed database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...</td>\n",
       "      <td>This video demonstrates the power of IBM DataScience Experience using a simple New Yor...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>[2345, 176, 457, 3011, 3207, 3302, 3827, 3986, 4179, 4231, 4239, 4308, 5040]</td>\n",
       "      <td>analyze ny restaurant data using spark in dsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    doc_body                                                                            doc_description  article_id  views                                                                             users_accessed                                                                  doc_name\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...                     Detect bad readings in real time using Python and Streaming Analytics.           0     14              [495, 495, 495, 503, 233, 552, 1347, 1051, 785, 2992, 3216, 3570, 4571, 4836]                detect malfunctioning iot sensors with streaming analytics\n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n * kaggle.com\\r\\n\\r\\nCommunicating data s...  See the forest, see the trees. Here lies the challenge in both performing and presenti...           1      0                                                                                         []               Communicating data science: A guide to presenting your work\n",
       "2  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Paths\\r\\n * Courses * Our Courses\\r\\n    * ...                                      Here’s this week’s news in Data Science and Big Data.           2     58  [676, 668, 668, 1145, 23, 23, 60, 60, 665, 98, 668, 794, 217, 60, 1401, 46, 1577, 789,...                                this week in data science (april 18, 2017)\n",
       "3  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCALE - BOOST THE PERFORMANCE OF YOUR\\r\\nDI...  Learn how distributed DBs solve the problem of scaling persistent storage, but introdu...           3      0                                                                                         []  DataLayer Conference: Boost the performance of your distributed database\n",
       "4  Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...  This video demonstrates the power of IBM DataScience Experience using a simple New Yor...           4     13               [2345, 176, 457, 3011, 3207, 3302, 3827, 3986, 4179, 4231, 4239, 4308, 5040]                             analyze ny restaurant data using spark in dsx"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the articles information dataframe\n",
    "users_per_article.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"Rank\">Utilitary Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>NOTES</b>:\n",
    "    <ul>\n",
    "<li>The dataset does not contain ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can be based on how often an article was interacted with.</li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Get the top n most popular articles</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the ids and title of the most viewed n articles\n",
    "def get_top_articles (n):\n",
    "    \n",
    "    '''\n",
    "    Finds the n most popular articles.\n",
    "    \n",
    "    INPUT:\n",
    "        n (int) - specifies how many items should be returned\n",
    "    OUTPUT:\n",
    "        article_ids (list) - the ids of the n most popular articles\n",
    "        titles (list) - the titles of the n most popular articles\n",
    "    '''\n",
    "    \n",
    "    df_top_n = users_per_article.nlargest(n, 'views')\n",
    "    titles = list(df_top_n.doc_name)\n",
    "    article_ids = list(df_top_n.article_id)\n",
    "    return article_ids, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1429, 1330, 1431, 1427, 1364]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ids of the 5 most popular articles\n",
    "get_top_articles(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use deep learning for image classification',\n",
       " 'insights from new york car accident reports',\n",
       " 'visualize car data with brunel',\n",
       " 'use xgboost, scikit-learn & ibm watson machine learning apis',\n",
       " 'predicting churn with the spss random tree algorithm']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The titles of the 5 most popular articles\n",
    "get_top_articles(5)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "\n",
    "`1.` Reformat the `df` dataframe to be shaped with users as the rows and articles as the columns.  \n",
    "\n",
    "* Each `user` appears in each row once.\n",
    "\n",
    "* Each `article` shows up in only one `column`.  \n",
    "\n",
    "* **If a user has interacted with an article, a 1 is placed where the user-row meets for that article-column**.  It does not matter how many times a user has interacted with the article, all entries where a user has interacted with an article are **1**.  \n",
    "\n",
    "* **If a user has not interacted with an item, a zero is placed where the user-row meets for that article-column**. \n",
    "\n",
    "The tests are used to make sure the basic structure of the matrix matches what is expected by the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df (pandas dataframe) - article_id, title, user_id are the columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item (nd array) - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns,\n",
    "    with 1 values where a user interacted with an article and a 0 otherwise.\n",
    "    '''\n",
    "    \n",
    "    # Create new column that keeps track of user_article interaction\n",
    "    df['interact'] = 1\n",
    "    # Transform df so that every user is on a row and every article corresponds to a column\n",
    "    user_item =  df.groupby(['user_id', 'article_id'])['interact'].first().unstack()\n",
    "    # Fill in NaN with 0 in the user_item matrix\n",
    "    user_item.fillna(0, inplace = True)\n",
    "    \n",
    "    return user_item # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` _The function below takes a `user_id` and provide an ordered list of the most similar users to that user (from most similar to least similar). The returned result should not contain the provided `user_id`, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. \n",
    "\n",
    "_We use the tests to test the function._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    # compute similarity of each user to the provided user\n",
    "    user_similarities = user_item.dot(user_item.loc[user_id])\n",
    "\n",
    "    # sort by similarity\n",
    "    sorted_similarities = user_similarities.sort_values(ascending=False)\n",
    "\n",
    "    # create list of just the ids\n",
    "    similars = list(sorted_similarities.index)\n",
    "   \n",
    "    # remove the own user's id\n",
    "    most_similar_users = similars[1:]\n",
    "       \n",
    "    return most_similar_users # return a list of the users in order from most to least similar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` _Now that we have a function that provides the most similar users to each user, we will want to use these users to find articles we can recommend.  The functions below return the articles we would recommend to each user._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    article_names = [df[df['article_id'] == float(x)]['title'].unique()[0] for x in article_ids]\n",
    "    # Return the article names associated with list of article ids\n",
    "    return article_names \n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "\n",
    "    article_ids = user_item.loc[user_id][user_item.loc[user_id] == 1].index.astype('str').to_list()\n",
    "    article_names = get_article_names(article_ids, df)\n",
    "    return article_ids, article_names # return the ids and names\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # articles_seen by user (we don't want to recommend these)\n",
    "    articles_seen = get_user_articles(user_id, user_item)[0]\n",
    "    # find the similar users\n",
    "    similar_users = find_similar_users(user_id)\n",
    "    \n",
    "    # list of recommended articles\n",
    "    recs = []\n",
    "    \n",
    "    for user in similar_users:\n",
    "        user_list = get_user_articles(user, user_item)[0]\n",
    "        recs_update = np.setdiff1d(user_list, articles_seen)\n",
    "        recs.extend(np.setdiff1d(recs_update, recs))\n",
    "     \n",
    "        if len(recs) >= m:\n",
    "            break\n",
    "    \n",
    "    return recs[:m] # return recommendations for this user_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Results\n",
    "get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the functions here - No need to change this code - just run this cell\n",
    "assert set(get_article_names(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names(['1320.0', '232.0', '844.0'])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set(['1320.0', '232.0', '844.0'])\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` _Now we are going to improve the consistency of the **user_user_recs** function from above._\n",
    "\n",
    "* _Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions._\n",
    "\n",
    "* _Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be what would be obtained from the **top_articles** function written earlier._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample user id\n",
    "user_id = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the other users based on similarity\n",
    "neighbor_id = find_similar_users(user_id)\n",
    "len(neighbor_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record the similarity measure, i.e. the dot product with user_id \n",
    "similarity = [user_item.loc[neighbor].dot(user_item.loc[user_id]) for neighbor in neighbor_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of views for each user\n",
    "num_interactions = [df[df['user_id'] == x].shape[0] for x in neighbor_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary \n",
    "neighbors_df = {'neighbor_id': neighbor_id,\n",
    "               'similarity': similarity,\n",
    "               'num_interactions': num_interactions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from dictionary\n",
    "neighbors_df = pd.DataFrame(data=neighbors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the output\n",
    "neighbors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the values\n",
    "neighbors_df.sort_values(by=['similarity', 'num_interactions'], ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the information of the user_id we started with\n",
    "neighbors_df.loc[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the most simlar users\n",
    "neighbor_list = neighbor_id[:10]\n",
    "neighbor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one member from the neighbors list\n",
    "user = neighbor_list[1]\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get the articles seen by the similar user\n",
    "similar_seen = get_user_articles(user, user_item)[0]\n",
    "len(similar_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the articles seen by user_id member\n",
    "articles_seen =  get_user_articles(user_id, user_item)[0]\n",
    "articles_seen[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the articles seen by user_id from neighbor's list\n",
    "# these are the articles to recommend\n",
    "articles_to_rec = np.setdiff1d(similar_seen, articles_seen)\n",
    "len(articles_to_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final list of articles to recommend\n",
    "recs_ids = []\n",
    "# the articles seen by similar user to add to the recommended list\n",
    "# remove those articles already in the list\n",
    "articles_to_add = np.setdiff1d(articles_to_rec, recs_ids)\n",
    "len(articles_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the next step we need article ids as float or integer\n",
    "articles_ids = [float(x) for x in articles_to_add]\n",
    "articles_ids[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the article ids\n",
    "df_reduced=df[df['article_id'].isin(articles_ids)]\n",
    "df_reduced.groupby('article_id').count()['title'].sort_values(ascending=False).index.to_list()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # order the other users based on similarity with member user_id\n",
    "    neighbor_id = find_similar_users(user_id)\n",
    "    # record the similarity measure, i.e. the dot product with user_id \n",
    "    similarity = [user_item.loc[neighbor].dot(user_item.loc[user_id]) for neighbor in neighbor_id]\n",
    "    # find the number of views/interactions for each user\n",
    "    num_interactions = [df[df['user_id'] == x].shape[0] for x in neighbor_id]\n",
    "    \n",
    "    # create a dataframe \n",
    "    neighbors_df = pd.DataFrame(data={'neighbor_id': neighbor_id,\n",
    "                                      'similarity': similarity,\n",
    "                                      'num_interactions': num_interactions})\n",
    "    # drop the row corresponding to the member user_id\n",
    "    neighbors_df.drop(user_id, axis = 0, inplace = True)\n",
    "    \n",
    "    # sort by similarity and num_interactions                 \n",
    "    neighbors_df.sort_values(by = ['similarity', 'num_interactions'], \n",
    "                             inplace=True, \n",
    "                             ascending=(False, False))   \n",
    "    \n",
    "    # Return the dataframe specified in the doc_string\n",
    "    return neighbors_df \n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # list of recommended articles by id, and by title\n",
    "    recs_ids = []\n",
    "    \n",
    "    # articles_seen by user (we don't want to recommend these)\n",
    "    articles_seen = get_user_articles(user_id, user_item)[0]\n",
    "    \n",
    "    # similar users with most article views\n",
    "    similar_users = get_top_sorted_users(user_id, df, user_item)\n",
    "    \n",
    "    for user in similar_users['neighbor_id'].values:\n",
    "        \n",
    "        # get the articles seen by the similar user\n",
    "        similar_seen = get_user_articles(user, user_item)[0]\n",
    "        \n",
    "        # remove the articles in articles_seen\n",
    "        articles_to_rec = np.setdiff1d(similar_seen, articles_seen)\n",
    "        \n",
    "        # remove the articles already added to the recs list\n",
    "        articles_to_add = np.setdiff1d(articles_to_rec, recs_ids)\n",
    "        \n",
    "        # rewrite the recommended article ids as float \n",
    "        articles_ids = [float(x) for x in articles_to_add]\n",
    "        \n",
    "        # sort the articles by popularity, i.e. number of views\n",
    "        df_red = df[df['article_id'].isin(articles_ids)]\n",
    "        sorted_articles=df_red.groupby('article_id').count()['title'].sort_values(ascending=False).index.to_list()\n",
    "       \n",
    "        # add the sorted article ids\n",
    "        recs_ids.extend(sorted_articles)\n",
    "        \n",
    "        # break when we have enough articles to recommend\n",
    "        if len(recs_ids) >= m:\n",
    "            break\n",
    "    \n",
    "    # retain the first m recommendations\n",
    "    recs = recs_ids[:m]\n",
    "    \n",
    "    # get the articles names\n",
    "    rec_names = get_article_names(recs, df)\n",
    "    \n",
    "    return recs, rec_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick spot check - don't change this code - just use it to test your functions\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` _Based on the functions from above to we fill in the solutions to the dictionary below. Then test the dictionary against the solution. The code needed to answer each of the following comments below is provided._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests with a dictionary of results\n",
    "\n",
    "# Find the user that is most similar to user 1 \n",
    "user1_most_sim = get_top_sorted_users(1, df, user_item).loc[0]['neighbor_id']\n",
    "# Find the 10th most similar user to user 131\n",
    "user131_10th_sim = get_top_sorted_users(131, df, user_item).loc[10]['neighbor_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user1_most_sim, user131_10th_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` _If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For a new use we recommend the most popular articles on the website, through `get_top_articles` and `get_top_article_ids` functions, as we don't have any information about user's preferences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` _Using the existing functions, we provide the top 10 recommended articles we would provide for a new user below. Test the function against the standard solution._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "new_user_recs = get_top_article_ids(10, df) # Your recommendations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite recommendations as a set of strings\n",
    "new_user_recs = [str(x) for x in new_user_recs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(new_user_recs) == set(['1314.0','1429.0','1293.0','1427.0',\n",
    "                                  '1162.0','1364.0','1304.0','1170.0','1431.0','1330.0']), \"Oops! It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations</a>\n",
    "\n",
    "Another method we might use to make recommendations is to perform a ranking of the highest ranked articles associated with some term. We could consider content to be the `doc_body`, `doc_description`, or `doc_full_name`.  There isn't one way to create a content based recommendation, especially considering that each of these columns hold content related information.  \n",
    "\n",
    "`1.` _We will create a content based recommender based on `doc_description` and `full_doc_title` columns._  \n",
    "\n",
    "`2.` _We choose the most popular recommendations that meet the content criteria._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copies of the data\n",
    "df_copy = df.copy()\n",
    "df_content_copy = df_content.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the data\n",
    "df_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data types in df\n",
    "df_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype of article id in df\n",
    "df_copy['article_id'] = df_copy['article_id'].astype('int')\n",
    "\n",
    "# check the output\n",
    "df_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the titles in df\n",
    "df_copy.title[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all titles in df dataframe\n",
    "df_copy['title'] = df_copy['title'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the content data\n",
    "df_content_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data types in df_content\n",
    "df_content_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a closer look at the data\n",
    "df_content_copy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column doc_full_name to title\n",
    "df_content_copy.rename(columns={'doc_full_name': 'title'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all the article titles\n",
    "df_content_copy['title'] = df_content_copy['title'].apply(lambda x: x.lower())\n",
    "\n",
    "# check the outcome\n",
    "df_content_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a dataframe that has  columns: title, doc_description and article_id \n",
    "df_cont = df_content_copy[['article_id', 'title']]\n",
    "\n",
    "# check the output\n",
    "df_cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for article titles in df\n",
    "df_titles = df_copy[['article_id', 'title']]\n",
    "\n",
    "df_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df_titles.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicates from dataframe df_titles\n",
    "df_titles = df_titles[~df_titles.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataframes so all available articles are included\n",
    "df_full = pd.merge(df_cont, df_titles,\n",
    "                  left_on = ['article_id', 'title'],\n",
    "                  right_on = ['article_id', 'title'],\n",
    "                  how = 'outer')\n",
    "\n",
    "# check the outcome\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the output\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process the Text and Create TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Contains the pre-processing steps for a document:\n",
    "        - tokenize\n",
    "        - lemmatize\n",
    "        - lowercasing\n",
    "        - removes stopwords in English language\n",
    "        \n",
    "    INPUT (string) - raw message\n",
    "    OUTPUT (list)  - clean tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove punctuation and unusual characters \n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text).strip()\n",
    "    \n",
    "    # split into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize - reduce words to their root form\n",
    "    words = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    \n",
    "    # case normalize and remove leading & trailing empty spaces\n",
    "    words = [w.lower().strip() for w in words]\n",
    "    \n",
    "    # remove stopwords, keep not and can\n",
    "    clean_words = [w for w in words if w not in stopwords.words('english') \n",
    "                   or w in ['not', 'can']]\n",
    "    \n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# construct the TF-IDF matrix \n",
    "tfidf_matrix = tfidf.fit_transform(df_full['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the shape of the tfidf matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the cosine similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cosine similarity matrix \n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "# check the output\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the recommender function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(article_id, df_full):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_id - (integer)\n",
    "    df_full - (pandas dataframe) contains article_id, title\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_title - (string) the article name associated with the provided article id\n",
    "    '''\n",
    "    \n",
    "    article_title = df_full[df_full['article_id']==article_id]['title'].unique()[0]\n",
    "    return article_id, article_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a sample output\n",
    "\n",
    "print('The title of the article with id = 542 is: {}.'.format(get_article_info(542, df_full)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in the article id as input and gives n recommendations\n",
    "def content_recommender(article_id, n, cosine_sim, df_full):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_id - (integer)\n",
    "    n - (integer) how many recommendations should be returned\n",
    "    cosine_sim  - (np.ndarray) matrix of cosine similarities\n",
    "    df - (pandas dataframe) contains title and article id\n",
    "    \n",
    "    OUTPUT:\n",
    "    recommended_articles  - (list) article ids and titles that are recommended, \n",
    "                             sorted by cosine similarity\n",
    "    '''\n",
    "    # get the information for the given article id\n",
    "    given_art = get_article_info(article_id, df_full)\n",
    "    \n",
    "    # obtain the matrix index that matches the article id\n",
    "    mat_index = df_full[df_full['article_id']==article_id].index.values[0]\n",
    "    \n",
    "    # sort the scores based on the cosine similarity scores with given article index, ignore the first entry\n",
    "    sim_scores = pd.Series(cosine_sim[mat_index]).sort_values(ascending=False).iloc[1:]\n",
    "    \n",
    "    # get the indices corresponding to the scores of the n most similar articles\n",
    "    sim_scores_n = list(sim_scores[:n+1].index.values)\n",
    "    \n",
    "    # return the top n most similar article_ids as a pandas dataframe\n",
    "    recommended_articles = df_full.iloc[sim_scores_n]\n",
    "    \n",
    "    return given_art, recommended_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give recommendations for the article with id 20\n",
    "article_id = 20\n",
    "rec_id20 = content_recommender(article_id, 10, cosine_sim, df_full)\n",
    "rec_id20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the article ids for the recommended articles for article_id=20\n",
    "rec_id20[1]['article_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make recommendations for article id 224 based on title\n",
    "content_recommender(224, 5, cosine_sim, df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Content Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_recs(user_id, cosine_sim, m=10, df=df, df_full=df_full):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    df - (pandas dataframe) contains user and articles interactions\n",
    "    df_full - (pandas dataframe) contains title and article id\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs_ids - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the articles based on closeness to the articles seen by the user.\n",
    "    For each article seen by the user - finds n most similar articles based on content recommendations.\n",
    "    Does this until m recommendations are found.\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the articles that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "   \n",
    "    '''\n",
    "    # list of recommended articles by id, and by title\n",
    "    recommendations = []\n",
    "    \n",
    "    # articles_seen by user \n",
    "    articles_seen = get_user_articles(user_id, user_item)[0]\n",
    "    \n",
    "    # rewrite the recommended article ids as int\n",
    "    articles_ids_seen = [int(x[:-2]) for x in articles_seen]\n",
    "    \n",
    "    for art_id in articles_ids_seen:\n",
    "        \n",
    "        # get the n most similar articles \n",
    "        n = 10\n",
    "        similar_articles = content_recommender(art_id, n, cosine_sim, df_full)[1]['article_id'].tolist()\n",
    "\n",
    "        # remove the articles in articles_seen and available\n",
    "        articles_to_rec = np.setdiff1d(similar_articles, articles_ids_seen)\n",
    "        \n",
    "        # remove the articles already added to the recs list\n",
    "        articles_to_add = np.setdiff1d(articles_to_rec, recommendations)\n",
    "        \n",
    "        # add the sorted article ids\n",
    "        recommendations.extend(articles_to_add)\n",
    "        \n",
    "        # break when we have enough articles to recommend\n",
    "        if len(recs_ids) >= m:\n",
    "            break\n",
    "    \n",
    "    # retain the first m recommendations\n",
    "    recs = recommendations[:m]\n",
    "    \n",
    "    # get the articles titles\n",
    "    complete_recs = [get_article_info(int(idx), df_full) for idx in recs]\n",
    "    \n",
    "    return complete_recs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles seen by user 40\n",
    "articles_seen = get_user_articles(40, user_item)[0]\n",
    "articles_ids_seen = [float(x) for x in articles_seen]\n",
    "articles_info = [get_article_info(index, df_full) for index in articles_ids_seen]\n",
    "articles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendations for user 40 based on title content\n",
    "make_content_recs(40, cosine_sim, 10, df, df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles seen by user 178\n",
    "articles_seen = get_user_articles(178, user_item)[0]\n",
    "articles_ids_seen = [int(x[:-2]) for x in articles_seen]\n",
    "articles_info = [get_article_info(index, df_full)[0] for index in articles_ids_seen]\n",
    "articles_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendations for user 178 based on title content\n",
    "make_content_recs(178, cosine_sim, 10, df, df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` _Now that you have put together your content-based recommendation system, use the cell below to write a summary explaining how your content based recommender works.  Do you see any possible improvements that could be made to your function?  Is there anything novel about your content based recommender?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The content based recommendations are based on the article title or on the article description. The corpus consists of one group of these documents. The text is processed by removing punctuation and stop words, it is lemmatized and split into tokens. Once processed the corpus is fed into a TdIdf Vectorizer that creates a matrix of scores. The cosine similarities between any two documents (rows in the similarity matrix) are computed and the results are saved in a 1051x1051 matrix.**\n",
    "\n",
    "**Given a user id, and assuming that the user has seen at least one article, the engine will recommend n content similar articles for each article seen by the user. Once this collection is created, it is sorted using the article popularity and m most popular articles are recommended.**\n",
    "\n",
    "**One way to improve these results is to use more efficient NLP techniques, such as word embedding. Another option would be to create meta data for the articles based on their descriptions and full text, both available in the `df_content` dataframe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` _Use your content-recommendation system to make recommendations for the below scenarios based on the comments.  Again no tests are provided here, because there isn't one right answer that could be used to find these content based recommendations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make recommendations for a brand new user - recommend the most popular articles\n",
    "new_user_recs = get_top_article_ids(10, df) \n",
    "new_user_recommendations = [get_article_info(art_id, df_full) for art_id in new_user_recs]\n",
    "new_user_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make recommendations for a user who only has interacted with article id '1427.0'\n",
    "content_recommender(1427, 10, cosine_sim, df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, we use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` _Upload the user_item matrix from part 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the matrix\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` _In this situation, you can use Singular Value Decomposition from [numpy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) on the user-item matrix.  Use the cell to perform SVD, and explain why this is different than in the lesson._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the shape of the user_item matrix\n",
    "user_item_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the User-Item Matrix Here\n",
    "\n",
    "u, s, vt = np.linalg.svd(user_item_matrix)# use the built in to get the three matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info on the output matrices\n",
    "u.shape, s.shape, vt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The user_movie_subset matrix contains the movie ratings as entries, so that the entry (ij) corresponds to the rating j given by the user i. In this approach we get numerous missing entries for those users who did not watch or rated a certain movie. Since the SVD decomposition does not work with missing values, it has be replaced by an alternate approach, such as FunkSVD.**\n",
    "\n",
    "**In the case of IBM recommendations, the user_item_matrix is a sparse array of binary entries, which records the interaction/no interaction between an user and an article. Thus, in this case we can apply SVD decomposition matrix method as the matrix does not have missing entries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` _Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Use the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below: \n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall the df size\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    '''\n",
    "    # create the two user_item matrices\n",
    "    user_item_train = create_user_item_matrix(df_train)\n",
    "    user_item_test = create_user_item_matrix(df_test)\n",
    "    \n",
    "    # extract the test user ids\n",
    "    test_idx = list(user_item_test.index)\n",
    "    # extract the test article_ids\n",
    "    test_arts = user_item_test.columns\n",
    "    \n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the shapes of the two matrices\n",
    "user_item_train.shape, user_item_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the user ids in test set, the article ids in test set\n",
    "len(test_idx), len(test_arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common user ids in the train and the test sets, \n",
    "common_users = len(set(user_item_test.index) & set(user_item_train.index))\n",
    "\n",
    "print('There are {} common user ids in the train and test sets.'.format(common_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of user ids in the test set\n",
    "test_idx = len(list(user_item_test.index))\n",
    "# the number of user ids in the test set that are not in the train set\n",
    "test_idx - common_users\n",
    "\n",
    "print('There are {} users in the test set that are not in the train set.'.format(test_idx - common_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of all article ids in the train set\n",
    "len(user_item_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of all article ids in the test set\n",
    "len(user_item_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common article ids in the train and the test sets\n",
    "common_articles = len(set(user_item_test.columns) & set(user_item_train.columns))\n",
    "\n",
    "print('There are {} common article ids in the train and test sets.'.format(common_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': c, \n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?': a, \n",
    "    'How many movies can we make predictions for in the test set?': b,\n",
    "    'How many movies in the test set are we not able to make predictions for because of the cold start problem?': d\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now use the **user_item_train** dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "Use the cells below to explore how well SVD works towards making predictions for recommendations on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SVD on the user_item_train matrix\n",
    "u_train, s_train, vt_train = np.linalg.svd(user_item_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shapes of the three matrices \n",
    "u_train.shape, s_train.shape, vt_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the users we can make predictions for\n",
    "common_user_ids = list(set(user_item_test.index) & set(user_item_train.index))\n",
    "# the articles we can use to make predictions\n",
    "test_arts = user_item_test.columns\n",
    "\n",
    "print(common_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the user_item_test to include only the common user_ids\n",
    "user_item_test_red = user_item_test.loc[common_user_ids, test_arts]\n",
    "# check the shape of the reduced matrix\n",
    "user_item_test_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset and relabel indices to identify the common user ids\n",
    "user_item_train_adj = user_item_train.reset_index()\n",
    "# relabel indices to avoid out of range error\n",
    "users_common_idx = user_item_train_adj[user_item_train_adj['user_id'].isin(common_user_ids)].index.to_list()\n",
    "print(users_common_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce vt to the articles in test set\n",
    "vt_test = vt_train[:, user_item_train.columns.isin(test_arts)]\n",
    "\n",
    "# reduce u to the common user ids - note: need to work with the updated indices\n",
    "u_test = u_train[users_common_idx, :]\n",
    "\n",
    "# check the shapes of the reduced matrices\n",
    "u_test.shape, vt_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs_train = []\n",
    "sum_errs_test = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features for the train set matrices\n",
    "    s_new_train, u_new_train, vt_new_train = np.diag(s_train[:k]), u_train[:, :k], vt_train[:k, :]\n",
    "    # restructure with k latent features for the test set matrices        \n",
    "    s_new_test, u_new_test, vt_new_test = np.diag(s_train[:k]), u_test[:, :k], vt_test[:k, :]\n",
    "    \n",
    "    # take dot products for each group\n",
    "    user_item_est_train = np.around(np.dot(np.dot(u_new_train, s_new_train), vt_new_train))\n",
    "    user_item_est_test = np.around(np.dot(np.dot(u_new_test, s_new_train), vt_new_test))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs_train = np.subtract(user_item_train, user_item_est_train)\n",
    "    diffs_test = np.subtract(user_item_test_red, user_item_est_test)\n",
    "    \n",
    "    # total train errors and keep track of them\n",
    "    err_train = np.sum(np.sum(np.abs(diffs_train)))\n",
    "    sum_errs_train.append(err_train)\n",
    "    \n",
    "    # total test errors and keep track of them\n",
    "    err_test = np.sum(np.sum(np.abs(diffs_test)))\n",
    "    sum_errs_test.append(err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the matrix shapes for k=10\n",
    "s_new_test, u_new_test, vt_new_test = np.diag(s_train[:10]), u_test[:, :10], vt_test[:10, :]\n",
    "s_new_train.shape, u_new_test.shape, vt_new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(num_latent_feats, 1 - np.array(sum_errs_test)/df.shape[0], color='b', label=\"Test accuracy\");\n",
    "ax2.plot(num_latent_feats, 1 - np.array(sum_errs_train)/df.shape[0], color = 'g', label=\"Train accuracy\");\n",
    "\n",
    "# get handlers and labels for the legend\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# create a legend for the test accuracy curve\n",
    "ax1.legend(h1+h2, l1+l2, loc='center right')\n",
    "\n",
    "ax1.set_xlabel('Number of Latent Features');\n",
    "ax2.set_ylabel('Train Accuracy');\n",
    "ax1.set_ylabel('Test Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make with any of the above recommendation systems are an improvement to how users currently find articles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The test accuracy decreases while the train accuracy increases with the number of latent features. The two accuracy curves intersect for 100 latent features. The shapes of the two curves indicate overfitting. The more latent features we use the more overfitted the model is. This is not surprising as the test set is quite small and thus easy to overfit.**\n",
    "\n",
    "\n",
    "**The following steps can be taken to improve the recommendation engine:**\n",
    "\n",
    "* **Design an A/B test to compare the warm users (those who receive content and collaborative filtering recommendations) with the cold users (the new users who are recommended the most popular articles only).** \n",
    "\n",
    "* **Given how imbalanced the data is, measures for the models's performance such as  F1 score and ROC/AUC, would provide a slight improvement in our estimations. Also we could take into consideration the number of unique articles a user interacts with.** \n",
    "\n",
    "* **Use different algorithms (such as the multi-arm bandit) for the cold user problem.**\n",
    "\n",
    "**However the efficiency of any method is still affected by the data imbalance and the best way to improve the results is to increase the size of the test set by collecting data from more users, after which we could implement the suggestions mentioned above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conclusions'></a>\n",
    "### Extras\n",
    "Using your workbook, you could now save your recommendations for each user, develop a class to make new predictions and update your results, and make a flask app to deploy your results.  These tasks are beyond what is required for this project.  However, from what you learned in the lessons, you certainly capable of taking these tasks on to improve upon your work here!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "> Congratulations!  You have reached the end of the Recommendations with IBM project! \n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the [rubric](https://review.udacity.com/#!/rubrics/2322/view). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
