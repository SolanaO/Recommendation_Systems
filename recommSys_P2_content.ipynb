{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='darkblue'> Recommendations with IBM Watson. Part II</font>\n",
    "\n",
    "## <font color='darkblue'> Rank and Content Based Recommendations</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, rank based recommendations to address the cold start problem and content based recommendations methods are investigated. The dataset is real data from the IBM Watson Studio platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Environment SetUp</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries and packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from ast import literal_eval\n",
    "\n",
    "# Recommendation systems library\n",
    "import surprise\n",
    "\n",
    "# Packages and libraries for content based recs\n",
    "import re\n",
    "\n",
    "# NLP packages\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Data processing packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import linear kernel to compute the dot product\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization packages and libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Choose style and color palette\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "colors = sns.color_palette('PuBuGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2 decimal places in output display\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# Don't wrap dataframe across additional lines\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "# Set the maximum widths of columns\n",
    "pd.set_option(\"display.max_colwidth\", 90)\n",
    "\n",
    "# Set max rows displayed in output to 20\n",
    "pd.set_option(\"display.max_rows\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Upload Preprocessed Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>NOTES</b>:\n",
    "    <ul>\n",
    "        <li>There are 4 files to upload.</li>\n",
    "        <li>Two files are centered on the user, the remaining two are centered on the article.</li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the user-item interaction files\n",
    "df = pd.read_csv('data/df.csv', index_col=[0])\n",
    "articles_per_user = pd.read_csv('data/articles_per_user.csv', \n",
    "                                index_col=[0], \n",
    "                                converters={'viewed_articles': literal_eval})\n",
    "\n",
    "# Read the articles information files\n",
    "df_content = pd.read_csv('data/articles_community.csv', index_col=[0])\n",
    "users_per_article = pd.read_csv('data/users_per_article.csv', \n",
    "                                index_col=[0],\n",
    "                               converters={'users_accessed': literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>viewed_articles</th>\n",
       "      <th>articles_count</th>\n",
       "      <th>unique_articles_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1430, 1430, 732, 1429, 43, 109, 1232, 310, 1293, 1406, 1406, 329, 585, 310, 1305, 105...</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1314, 1305, 1024, 1176, 1422, 1427]</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1429, 1429, 1330, 213, 1172, 1431, 1429, 1059, 1057, 29, 788, 1172, 868, 12, 1429, 10...</td>\n",
       "      <td>82</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1338, 1314, 1330, 1330, 1427, 1160, 1162, 1391, 1162, 887, 1420, 1394, 1305, 1314, 13...</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1276, 1351, 1166, 1351, 1351]</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                                                            viewed_articles  articles_count  unique_articles_count\n",
       "0        1  [1430, 1430, 732, 1429, 43, 109, 1232, 310, 1293, 1406, 1406, 329, 585, 310, 1305, 105...              47                     36\n",
       "1        2                                                       [1314, 1305, 1024, 1176, 1422, 1427]               6                      6\n",
       "2        3  [1429, 1429, 1330, 213, 1172, 1431, 1429, 1059, 1057, 29, 788, 1172, 868, 12, 1429, 10...              82                     40\n",
       "3        4  [1338, 1314, 1330, 1330, 1427, 1160, 1162, 1391, 1162, 887, 1420, 1394, 1305, 1314, 13...              45                     26\n",
       "4        5                                                             [1276, 1351, 1166, 1351, 1351]               5                      3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the user_item dataframes\n",
    "articles_per_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>article_id</th>\n",
       "      <th>views</th>\n",
       "      <th>users_accessed</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...</td>\n",
       "      <td>Detect bad readings in real time using Python and Streaming Analytics.</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>[495, 495, 495, 503, 233, 552, 1347, 1051, 785, 2992, 3216, 3570, 4571, 4836]</td>\n",
       "      <td>detect malfunctioning iot sensors with streaming analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n * kaggle.com\\r\\n\\r\\nCommunicating data s...</td>\n",
       "      <td>See the forest, see the trees. Here lies the challenge in both performing and presenti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Communicating data science: A guide to presenting your work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Paths\\r\\n * Courses * Our Courses\\r\\n    * ...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Big Data.</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>[676, 668, 668, 1145, 23, 23, 60, 60, 665, 98, 668, 794, 217, 60, 1401, 46, 1577, 789,...</td>\n",
       "      <td>this week in data science (april 18, 2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCALE - BOOST THE PERFORMANCE OF YOUR\\r\\nDI...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of scaling persistent storage, but introdu...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>DataLayer Conference: Boost the performance of your distributed database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...</td>\n",
       "      <td>This video demonstrates the power of IBM DataScience Experience using a simple New Yor...</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>[2345, 176, 457, 3011, 3207, 3302, 3827, 3986, 4179, 4231, 4239, 4308, 5040]</td>\n",
       "      <td>analyze ny restaurant data using spark in dsx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    doc_body                                                                            doc_description  article_id  views                                                                             users_accessed                                                                  doc_name\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...                     Detect bad readings in real time using Python and Streaming Analytics.           0     14              [495, 495, 495, 503, 233, 552, 1347, 1051, 785, 2992, 3216, 3570, 4571, 4836]                detect malfunctioning iot sensors with streaming analytics\n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n * kaggle.com\\r\\n\\r\\nCommunicating data s...  See the forest, see the trees. Here lies the challenge in both performing and presenti...           1      0                                                                                         []               Communicating data science: A guide to presenting your work\n",
       "2  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Paths\\r\\n * Courses * Our Courses\\r\\n    * ...                                      Here’s this week’s news in Data Science and Big Data.           2     58  [676, 668, 668, 1145, 23, 23, 60, 60, 665, 98, 668, 794, 217, 60, 1401, 46, 1577, 789,...                                this week in data science (april 18, 2017)\n",
       "3  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCALE - BOOST THE PERFORMANCE OF YOUR\\r\\nDI...  Learn how distributed DBs solve the problem of scaling persistent storage, but introdu...           3      0                                                                                         []  DataLayer Conference: Boost the performance of your distributed database\n",
       "4  Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...  This video demonstrates the power of IBM DataScience Experience using a simple New Yor...           4     13               [2345, 176, 457, 3011, 3207, 3302, 3827, 3986, 4179, 4231, 4239, 4308, 5040]                             analyze ny restaurant data using spark in dsx"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the articles information dataframe\n",
    "users_per_article.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"Rank\">Rank-Based Recommendations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>NOTES</b>:\n",
    "    <ul>\n",
    "<li>The dataset does not contain ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can be based on how often an article was interacted with.</li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Get the top n most popular articles</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the ids and title of the most viewed n articles\n",
    "def get_top_articles(n):\n",
    "    \n",
    "    '''\n",
    "    Finds the n most popular articles.\n",
    "    \n",
    "    INPUT:\n",
    "        n (int) - specifies how many items should be returned\n",
    "    OUTPUT:\n",
    "        article_ids (list) - the ids of the n most popular articles\n",
    "        titles (list) - the titles of the n most popular articles\n",
    "    '''\n",
    "    \n",
    "    df_top_n = users_per_article.nlargest(n, 'views')\n",
    "    titles = list(df_top_n.doc_name)\n",
    "    article_ids = list(df_top_n.article_id)\n",
    "    return article_ids, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1429, 1330, 1431, 1427, 1364]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ids of the 5 most popular articles\n",
    "get_top_articles(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use deep learning for image classification',\n",
       " 'insights from new york car accident reports',\n",
       " 'visualize car data with brunel',\n",
       " 'use xgboost, scikit-learn & ibm watson machine learning apis',\n",
       " 'predicting churn with the spss random tree algorithm']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The titles of the 5 most popular articles\n",
    "get_top_articles(5)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"Content-Recs\">Content Based Recommendations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Baseline Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>NOTES</b>:\n",
    "    <ul>\n",
    "<li>In this section we work with users_per_article dataframe that contains content information.</li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>article_id</th>\n",
       "      <th>views</th>\n",
       "      <th>users_accessed</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...</td>\n",
       "      <td>Detect bad readings in real time using Python and Streaming Analytics.</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>[495, 495, 495, 503, 233, 552, 1347, 1051, 785, 2992, 3216, 3570, 4571, 4836]</td>\n",
       "      <td>detect malfunctioning iot sensors with streaming analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n * kaggle.com\\r\\n\\r\\nCommunicating data s...</td>\n",
       "      <td>See the forest, see the trees. Here lies the challenge in both performing and presenti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Communicating data science: A guide to presenting your work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    doc_body                                                                            doc_description  article_id  views                                                                 users_accessed                                                     doc_name\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r\\nClose Yeah, keep it Undo CloseTHIS VID...                     Detect bad readings in real time using Python and Streaming Analytics.           0     14  [495, 495, 495, 503, 233, 552, 1347, 1051, 785, 2992, 3216, 3570, 4571, 4836]   detect malfunctioning iot sensors with streaming analytics\n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n * kaggle.com\\r\\n\\r\\nCommunicating data s...  See the forest, see the trees. Here lies the challenge in both performing and presenti...           1      0                                                                             []  Communicating data science: A guide to presenting your work"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a copy of the dataframe\n",
    "content = users_per_article.copy()\n",
    "\n",
    "# Take a look at the data\n",
    "content.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_body           291\n",
       "doc_description    280\n",
       "article_id           0\n",
       "views                0\n",
       "users_accessed       0\n",
       "doc_name             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "content.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>detect malfunctioning iot sensors with streaming analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Communicating data science: A guide to presenting your work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                                     doc_name\n",
       "0           0   detect malfunctioning iot sensors with streaming analytics\n",
       "1           1  Communicating data science: A guide to presenting your work"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe that retains article_id and doc_name \n",
    "df_titles = content[['article_id', 'doc_name']]\n",
    "\n",
    "# check the output\n",
    "df_titles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Preprocess text and create TF-IDF matrix</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Contains the pre-processing steps for a document:\n",
    "        - tokenize\n",
    "        - lemmatize\n",
    "        - lowercasing\n",
    "        - removes stopwords in English language\n",
    "        \n",
    "    INPUT (string) - raw message\n",
    "    OUTPUT (list)  - clean tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove punctuation and unusual characters \n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text).strip()\n",
    "    \n",
    "    # split into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize - reduce words to their root form\n",
    "    words = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    \n",
    "    # case normalize and remove leading & trailing empty spaces\n",
    "    words = [w.lower().strip() for w in words]\n",
    "    \n",
    "    # remove stopwords, keep not and can\n",
    "    clean_words = [w for w in words if w not in stopwords.words('english') \n",
    "                   or w in ['not', 'can']]\n",
    "    \n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# Construct the TF-IDF matrix \n",
    "tfidf_matrix = tfidf.fit_transform(df_titles['doc_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328, 1946)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the shape of the TF-IDF matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Compute the cosine similarity scores</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1328, 1328)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the cosine similarity matrix \n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "# Check the output\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Build the recommender function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(article_id, df_titles):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_id (int) - unique article identifier\n",
    "    df_titles (pd.DataFrame) - contains article_id, title\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_title (str) - the article name associated with the provided article id\n",
    "    '''\n",
    "    \n",
    "    article_title = df_titles[df_titles['article_id']==article_id]['doc_name'].unique()[0]\n",
    "    return article_id, article_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the article with id = 542 is: getting started with python.\n"
     ]
    }
   ],
   "source": [
    "# Print a sample output\n",
    "print(f'The title of the article with id = 542 is: {get_article_info(542, df_titles)[1]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes in the article id as input and gives n recommendations\n",
    "def content_recommender(article_id, n, cosine_sim, df_titles):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_id (int) - unique article identifier\n",
    "    n (int) - how many recommendations should be returned\n",
    "    cosine_sim (np.ndarray) - matrix of cosine similarities\n",
    "    df (pd.DdataFrame) - contains title and article id\n",
    "    \n",
    "    OUTPUT:\n",
    "    recommended_articles (list) - recommended articles (ids and titles), \n",
    "                                  sorted by cosine similarity\n",
    "    '''\n",
    "    # Get the information for the given article id\n",
    "    given_article = get_article_info(article_id, df_titles)\n",
    "    \n",
    "    # Obtain the matrix index that matches the article id\n",
    "    matrix_index = df_titles[df_titles['article_id']==article_id].index.values[0]\n",
    "    \n",
    "    # Sort the scores based on the cosine similarity scores with given article index, ignore the first entry\n",
    "    sim_scores = pd.Series(cosine_sim[matrix_index]).sort_values(ascending=False).iloc[1:]\n",
    "    \n",
    "    # Get the indices corresponding to the scores of the n most similar articles\n",
    "    sim_scores_n = list(sim_scores[:n+1].index.values)\n",
    "    \n",
    "    # Return the top n most similar article_ids as a pandas dataframe\n",
    "    recommended_articles = df_titles.iloc[sim_scores_n]\n",
    "    \n",
    "    return given_article, recommended_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article with id = 20 for which we give recommendations is:\n",
      "working interactively with rstudio and notebooks in dsx\n",
      "\n",
      "The recommended articles are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>373</td>\n",
       "      <td>working with notebooks in dsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>763</td>\n",
       "      <td>load data into rstudio for analysis in dsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>182</td>\n",
       "      <td>Overview of RStudio IDE in DSX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>355</td>\n",
       "      <td>run shiny applications in rstudio in dsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>626</td>\n",
       "      <td>analyze db2 warehouse on cloud data in rstudio in dsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>665</td>\n",
       "      <td>get social with your notebooks in dsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>958</td>\n",
       "      <td>using dsx notebooks to analyze github data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>474</td>\n",
       "      <td>publish notebooks to github in dsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>930</td>\n",
       "      <td>how to use version control (github) in rstudio within dsx?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>821</td>\n",
       "      <td>using rstudio in ibm data science experience</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                                    doc_name\n",
       "373         373                               working with notebooks in dsx\n",
       "763         763                  load data into rstudio for analysis in dsx\n",
       "182         182                              Overview of RStudio IDE in DSX\n",
       "355         355                    run shiny applications in rstudio in dsx\n",
       "626         626       analyze db2 warehouse on cloud data in rstudio in dsx\n",
       "665         665                       get social with your notebooks in dsx\n",
       "958         958                  using dsx notebooks to analyze github data\n",
       "474         474                          publish notebooks to github in dsx\n",
       "930         930  how to use version control (github) in rstudio within dsx?\n",
       "821         821                using rstudio in ibm data science experience"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the id for the test article\n",
    "article_id = 20\n",
    "\n",
    "# Create the list of recommendations\n",
    "recommendations_list_20 = content_recommender(article_id, 9, cosine_sim, df_titles)\n",
    "\n",
    "# Print the information for the test article\n",
    "print(f'The article with id = {article_id} for which we give recommendations is:\\n{recommendations_list_20[0][1]}')\n",
    "\n",
    "# Print the recommended articles information\n",
    "print(f'\\nThe recommended articles are:')\n",
    "recommendations_list_20[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article with id = 500 for which we give recommendations is:\n",
      "the difference between ai, machine learning, and deep learning?\n",
      "\n",
      "The recommended articles are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>313</td>\n",
       "      <td>what is machine learning?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>260</td>\n",
       "      <td>the machine learning database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>762</td>\n",
       "      <td>From Machine Learning to Learning Machine (Dinesh Nirmal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>deep learning with data science experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>1035</td>\n",
       "      <td>machine learning for the enterprise.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>800</td>\n",
       "      <td>machine learning for the enterprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>336</td>\n",
       "      <td>challenges in deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>337</td>\n",
       "      <td>generalization in deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1004</td>\n",
       "      <td>how to get a job in deep learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>809</td>\n",
       "      <td>use the machine learning library</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id                                                   doc_name\n",
       "313          313                                  what is machine learning?\n",
       "260          260                              the machine learning database\n",
       "762          762  From Machine Learning to Learning Machine (Dinesh Nirmal)\n",
       "237          237                 deep learning with data science experience\n",
       "1035        1035                       machine learning for the enterprise.\n",
       "800          800                        machine learning for the enterprise\n",
       "336          336                                challenges in deep learning\n",
       "337          337                            generalization in deep learning\n",
       "1004        1004                          how to get a job in deep learning\n",
       "809          809                           use the machine learning library"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the id for the test article\n",
    "article_id = 500\n",
    "\n",
    "# Create the list of recommendations\n",
    "recommendations_list_500 = content_recommender(article_id, 9, cosine_sim, df_titles)\n",
    "\n",
    "# Print the information for the test article\n",
    "print(f'The article with id = {article_id} for which we give recommendations is:\\n{recommendations_list_500[0][1]}')\n",
    "\n",
    "# Print the recommended articles information\n",
    "print(f'\\nThe recommended articles are:')\n",
    "recommendations_list_500[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<b>NOTES</b>:\n",
    "    <ul>\n",
    "<li>The two lists of recommendations look good.</li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Make content recommendations</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_recs(user_id, cosine_sim, m=20, df=articles_per_user, df_content=users_per_article):\n",
    "    '''\n",
    "    INPUT:\n",
    "        user_id (int) - unique numeric user identifier\n",
    "        m (int) - the number of recommendations we want for the user\n",
    "        df (pd.DataFrame) - contains users and articles interactions\n",
    "        df_content (pd.DataFrame) - contains titles and article_ids\n",
    "    \n",
    "    OUTPUT:\n",
    "        recs_ids (list) - list of recommendations for the user by article id\n",
    "        rec_names (list) - list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "        Loops through the articles based on closeness to the articles seen by the user.\n",
    "        For each article seen by the user - finds n most similar articles based on \n",
    "        content recommendations. Does this until m recommendations are found.\n",
    "    \n",
    "    Notes:\n",
    "        The articles that have the most total article interactions are chosen first.\n",
    "   \n",
    "    '''\n",
    "    # List of recommended articles by id, and by title\n",
    "    recommendations = []\n",
    "    \n",
    "    # Ids of articles seen by user  \n",
    "    articles_ids_seen = articles_per_user.loc[user_id].viewed_articles\n",
    "    \n",
    "    for art_id in articles_ids_seen:\n",
    "        \n",
    "        # get the n most similar articles ids and titles\n",
    "        n = 10\n",
    "        \n",
    "        similar_articles_ids = content_recommender(art_id, n, cosine_sim, df_titles)[1]['article_id'].tolist()\n",
    "        similar_articles = content_recommender(art_id, n, cosine_sim, df_titles)[1]['doc_name'].tolist()\n",
    "\n",
    "        # remove the ids of the articles in articles_ids_seen and available\n",
    "        articles_ids_to_recommend = np.setdiff1d(similar_articles_ids, articles_ids_seen)\n",
    "        \n",
    "        # remove the articles already added to the recs list\n",
    "        articles_ids_to_add = np.setdiff1d(articles_ids_to_recommend, recommendations)\n",
    "        \n",
    "        # add the sorted article ids\n",
    "        recommendations.extend(articles_ids_to_add)\n",
    "        \n",
    "        # break when we have enough articles to recommend\n",
    "        if len(recommendations) >= m:\n",
    "            break\n",
    "    \n",
    "    # retain the first m recommendations\n",
    "    recs = recommendations[:m]\n",
    "    \n",
    "    # get the articles titles\n",
    "    complete_recs = [get_article_info(article_id, df_titles) for article_id in recs]\n",
    "    \n",
    "    return complete_recs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles accessed by user_id=40 are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(151, 'jupyter notebook tutorial'),\n",
       " (162, 'an introduction to stock market data analysis with r (part 1)'),\n",
       " (486, 'use spark r to load and analyze data'),\n",
       " (542, 'getting started with python'),\n",
       " (645, 'how to perform a logistic regression in r'),\n",
       " (692, '15 page tutorial for r'),\n",
       " (1198, 'country statistics: commercial bank prime lending rate'),\n",
       " (1304, 'gosales transactions for logistic regression model'),\n",
       " (1368, 'putting a human face on machine learning'),\n",
       " (1430,\n",
       "  'using pixiedust for fast, flexible, and easier data analysis and experimentation'),\n",
       " (1436, 'welcome to pixiedust')}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the unique articles seen by user_id=40\n",
    "articles_ids_seen_40 = articles_per_user.loc[40].viewed_articles\n",
    "articles_seen_40 = [get_article_info(article_id, df_titles) for article_id in articles_ids_seen_40]\n",
    "print(f'The articles accessed by user_id=40 are:')\n",
    "set(articles_seen_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles recommended to user_id=40 are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(16, 'higher-order logistic regression for large datasets'),\n",
       " (82, 'build a logistic regression model with wml & dsx'),\n",
       " (609, 'simple linear regression? do it the bayesian way'),\n",
       " (751, 'build a predictive analytic model'),\n",
       " (1047, 'a comparison of logistic regression and naive bayes '),\n",
       " (1051, 'a tensorflow regression model to predict house values'),\n",
       " (1274, 'data model with streaming analytics and python'),\n",
       " (1276, 'deploy your python model as a restful api'),\n",
       " (1305, 'gosales transactions for naive bayes model'),\n",
       " (1350, 'model a golomb ruler'),\n",
       " (21,\n",
       "  'Mapping for Data Science with PixieDust and Mapbox – IBM Watson Data Lab – Medium'),\n",
       " (108,\n",
       "  '520    using notebooks with pixiedust for fast, flexi...\\nName: title, dtype: object'),\n",
       " (110, 'pixiedust: magic for your python notebook'),\n",
       " (522, 'share the (pixiedust) magic – ibm watson data lab – medium'),\n",
       " (617, 'pixiedust gets its first community-driven feature in 1.0.4'),\n",
       " (681,\n",
       "  'real-time sentiment analysis of twitter hashtags with spark (+ pixiedust)'),\n",
       " (729, 'pixiedust 1.0 is here! – ibm watson data lab'),\n",
       " (1039, 'You Too Can Make Magic (in Jupyter Notebooks with PixieDust)'),\n",
       " (1161, 'analyze data, build a dashboard with spark and pixiedust'),\n",
       " (1163, 'analyze open data sets with spark & pixiedust')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the list of recommended articles\n",
    "print(f'The articles recommended to user_id=40 are:')\n",
    "make_content_recs(40, cosine_sim, 20, articles_per_user, users_per_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles accessed by user_id=178 are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(173, '10 must attend data science, ml and ai conferences in 2018')}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the unique articles seen by user_id=178\n",
    "articles_ids_seen_178 = articles_per_user.loc[178].viewed_articles\n",
    "articles_seen_178 = [get_article_info(article_id, df_titles) for article_id in articles_ids_seen_178]\n",
    "print(f'The articles accessed by user_id=178 are:')\n",
    "set(articles_seen_178)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles recommended to user_id=178 are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(508, 'data science in the cloud'),\n",
       " (528, '10 tips on using jupyter notebook'),\n",
       " (661, '21 Must-Know Data Science Interview Questions and Answers'),\n",
       " (679, 'this week in data science'),\n",
       " (715,\n",
       "  \"for ai to get creative, it must learn the rules--then how to break 'em\"),\n",
       " (784, '10 data science, machine learning and ai podcasts you must listen to'),\n",
       " (878, '10 data science podcasts you need to be listening to right now'),\n",
       " (967, 'ml algorithm != learning machine'),\n",
       " (986, 'r for data science'),\n",
       " (990, 'this week in data science (january 10, 2017)'),\n",
       " (1177, 'cifar-10 - python version')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the list of recommended articles\n",
    "print(f'The articles recommended to user_id=178 are:')\n",
    "make_content_recs(178, cosine_sim, 20, articles_per_user, users_per_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` _Now that you have put together your content-based recommendation system, use the cell below to write a summary explaining how your content based recommender works.  Do you see any possible improvements that could be made to your function?  Is there anything novel about your content based recommender?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The content based recommendations are based on the article title or on the article description. The corpus consists of one group of these documents. The text is processed by removing punctuation and stop words, it is lemmatized and split into tokens. Once processed the corpus is fed into a TdIdf Vectorizer that creates a matrix of scores. The cosine similarities between any two documents (rows in the similarity matrix) are computed and the results are saved in a 1051x1051 matrix.**\n",
    "\n",
    "**Given a user id, and assuming that the user has seen at least one article, the engine will recommend n content similar articles for each article seen by the user. Once this collection is created, it is sorted using the article popularity and m most popular articles are recommended.**\n",
    "\n",
    "**One way to improve these results is to use more efficient NLP techniques, such as word embedding. Another option would be to create meta data for the articles based on their descriptions and full text, both available in the `df_content` dataframe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` _Use your content-recommendation system to make recommendations for the below scenarios based on the comments.  Again no tests are provided here, because there isn't one right answer that could be used to find these content based recommendations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make recommendations for a brand new user - recommend the most popular articles\n",
    "new_user_recs = get_top_article_ids(10, df) \n",
    "new_user_recommendations = [get_article_info(art_id, df_full) for art_id in new_user_recs]\n",
    "new_user_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make recommendations for a user who only has interacted with article id '1427.0'\n",
    "content_recommender(1427, 10, cosine_sim, df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, we use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` _Upload the user_item matrix from part 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the matrix\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` _In this situation, you can use Singular Value Decomposition from [numpy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) on the user-item matrix.  Use the cell to perform SVD, and explain why this is different than in the lesson._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the shape of the user_item matrix\n",
    "user_item_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the User-Item Matrix Here\n",
    "\n",
    "u, s, vt = np.linalg.svd(user_item_matrix)# use the built in to get the three matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info on the output matrices\n",
    "u.shape, s.shape, vt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The user_movie_subset matrix contains the movie ratings as entries, so that the entry (ij) corresponds to the rating j given by the user i. In this approach we get numerous missing entries for those users who did not watch or rated a certain movie. Since the SVD decomposition does not work with missing values, it has be replaced by an alternate approach, such as FunkSVD.**\n",
    "\n",
    "**In the case of IBM recommendations, the user_item_matrix is a sparse array of binary entries, which records the interaction/no interaction between an user and an article. Thus, in this case we can apply SVD decomposition matrix method as the matrix does not have missing entries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` _Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Use the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below: \n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall the df size\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    '''\n",
    "    # create the two user_item matrices\n",
    "    user_item_train = create_user_item_matrix(df_train)\n",
    "    user_item_test = create_user_item_matrix(df_test)\n",
    "    \n",
    "    # extract the test user ids\n",
    "    test_idx = list(user_item_test.index)\n",
    "    # extract the test article_ids\n",
    "    test_arts = user_item_test.columns\n",
    "    \n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the shapes of the two matrices\n",
    "user_item_train.shape, user_item_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the user ids in test set, the article ids in test set\n",
    "len(test_idx), len(test_arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common user ids in the train and the test sets, \n",
    "common_users = len(set(user_item_test.index) & set(user_item_train.index))\n",
    "\n",
    "print('There are {} common user ids in the train and test sets.'.format(common_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of user ids in the test set\n",
    "test_idx = len(list(user_item_test.index))\n",
    "# the number of user ids in the test set that are not in the train set\n",
    "test_idx - common_users\n",
    "\n",
    "print('There are {} users in the test set that are not in the train set.'.format(test_idx - common_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of all article ids in the train set\n",
    "len(user_item_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of all article ids in the test set\n",
    "len(user_item_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common article ids in the train and the test sets\n",
    "common_articles = len(set(user_item_test.columns) & set(user_item_train.columns))\n",
    "\n",
    "print('There are {} common article ids in the train and test sets.'.format(common_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': c, \n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?': a, \n",
    "    'How many movies can we make predictions for in the test set?': b,\n",
    "    'How many movies in the test set are we not able to make predictions for because of the cold start problem?': d\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now use the **user_item_train** dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "Use the cells below to explore how well SVD works towards making predictions for recommendations on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SVD on the user_item_train matrix\n",
    "u_train, s_train, vt_train = np.linalg.svd(user_item_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shapes of the three matrices \n",
    "u_train.shape, s_train.shape, vt_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the users we can make predictions for\n",
    "common_user_ids = list(set(user_item_test.index) & set(user_item_train.index))\n",
    "# the articles we can use to make predictions\n",
    "test_arts = user_item_test.columns\n",
    "\n",
    "print(common_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the user_item_test to include only the common user_ids\n",
    "user_item_test_red = user_item_test.loc[common_user_ids, test_arts]\n",
    "# check the shape of the reduced matrix\n",
    "user_item_test_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset and relabel indices to identify the common user ids\n",
    "user_item_train_adj = user_item_train.reset_index()\n",
    "# relabel indices to avoid out of range error\n",
    "users_common_idx = user_item_train_adj[user_item_train_adj['user_id'].isin(common_user_ids)].index.to_list()\n",
    "print(users_common_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce vt to the articles in test set\n",
    "vt_test = vt_train[:, user_item_train.columns.isin(test_arts)]\n",
    "\n",
    "# reduce u to the common user ids - note: need to work with the updated indices\n",
    "u_test = u_train[users_common_idx, :]\n",
    "\n",
    "# check the shapes of the reduced matrices\n",
    "u_test.shape, vt_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs_train = []\n",
    "sum_errs_test = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features for the train set matrices\n",
    "    s_new_train, u_new_train, vt_new_train = np.diag(s_train[:k]), u_train[:, :k], vt_train[:k, :]\n",
    "    # restructure with k latent features for the test set matrices        \n",
    "    s_new_test, u_new_test, vt_new_test = np.diag(s_train[:k]), u_test[:, :k], vt_test[:k, :]\n",
    "    \n",
    "    # take dot products for each group\n",
    "    user_item_est_train = np.around(np.dot(np.dot(u_new_train, s_new_train), vt_new_train))\n",
    "    user_item_est_test = np.around(np.dot(np.dot(u_new_test, s_new_train), vt_new_test))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs_train = np.subtract(user_item_train, user_item_est_train)\n",
    "    diffs_test = np.subtract(user_item_test_red, user_item_est_test)\n",
    "    \n",
    "    # total train errors and keep track of them\n",
    "    err_train = np.sum(np.sum(np.abs(diffs_train)))\n",
    "    sum_errs_train.append(err_train)\n",
    "    \n",
    "    # total test errors and keep track of them\n",
    "    err_test = np.sum(np.sum(np.abs(diffs_test)))\n",
    "    sum_errs_test.append(err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the matrix shapes for k=10\n",
    "s_new_test, u_new_test, vt_new_test = np.diag(s_train[:10]), u_test[:, :10], vt_test[:10, :]\n",
    "s_new_train.shape, u_new_test.shape, vt_new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(num_latent_feats, 1 - np.array(sum_errs_test)/df.shape[0], color='b', label=\"Test accuracy\");\n",
    "ax2.plot(num_latent_feats, 1 - np.array(sum_errs_train)/df.shape[0], color = 'g', label=\"Train accuracy\");\n",
    "\n",
    "# get handlers and labels for the legend\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# create a legend for the test accuracy curve\n",
    "ax1.legend(h1+h2, l1+l2, loc='center right')\n",
    "\n",
    "ax1.set_xlabel('Number of Latent Features');\n",
    "ax2.set_ylabel('Train Accuracy');\n",
    "ax1.set_ylabel('Test Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make with any of the above recommendation systems are an improvement to how users currently find articles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The test accuracy decreases while the train accuracy increases with the number of latent features. The two accuracy curves intersect for 100 latent features. The shapes of the two curves indicate overfitting. The more latent features we use the more overfitted the model is. This is not surprising as the test set is quite small and thus easy to overfit.**\n",
    "\n",
    "\n",
    "**The following steps can be taken to improve the recommendation engine:**\n",
    "\n",
    "* **Design an A/B test to compare the warm users (those who receive content and collaborative filtering recommendations) with the cold users (the new users who are recommended the most popular articles only).** \n",
    "\n",
    "* **Given how imbalanced the data is, measures for the models's performance such as  F1 score and ROC/AUC, would provide a slight improvement in our estimations. Also we could take into consideration the number of unique articles a user interacts with.** \n",
    "\n",
    "* **Use different algorithms (such as the multi-arm bandit) for the cold user problem.**\n",
    "\n",
    "**However the efficiency of any method is still affected by the data imbalance and the best way to improve the results is to increase the size of the test set by collecting data from more users, after which we could implement the suggestions mentioned above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conclusions'></a>\n",
    "### Extras\n",
    "Using your workbook, you could now save your recommendations for each user, develop a class to make new predictions and update your results, and make a flask app to deploy your results.  These tasks are beyond what is required for this project.  However, from what you learned in the lessons, you certainly capable of taking these tasks on to improve upon your work here!\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "> Congratulations!  You have reached the end of the Recommendations with IBM project! \n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the [rubric](https://review.udacity.com/#!/rubrics/2322/view). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "\n",
    "## Directions to Submit\n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of article information data\n",
    "metadata = users_per_article.copy()\n",
    "\n",
    "# Replace NaN with the string 'none'\n",
    "metadata.fillna('none', inplace=True)\n",
    "\n",
    "# Create a new column that combines doc_name and doc_description \n",
    "metadata['doc_text'] = metadata['doc_description'] +  ' , ' + metadata['doc_name']\n",
    "\n",
    "# Install packages to generate the BERT embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Transform the documents into 768-dim real vectors\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "metadata['vectors'] = metadata['doc_name'].apply(lambda x: model.encode(x))\n",
    "\n",
    "\n",
    "# Calculate Within Clusters SS for a range of k values\n",
    "# Adapted from: https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function returns WSS score for k values from 1 to kmax\n",
    "\n",
    "def calculate_WSS(points, kmax):\n",
    "    \n",
    "    sse = []\n",
    "    \n",
    "    for k in range(4, kmax+1):\n",
    "        kmeans = KMeans(n_clusters = k).fit(points)\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        pred_clusters = kmeans.predict(points)\n",
    "        curr_sse = 0\n",
    "        \n",
    "        # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
    "        for i in range(len(points)):\n",
    "            curr_center = centroids[pred_clusters[i]]\n",
    "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "        sse.append(curr_sse)\n",
    "    return sse\n",
    "\n",
    "# Create a list of vectors for Kmeans model\n",
    "X = np.array(metadata['vectors'].tolist())\n",
    "\n",
    "TOKENIZERS_PARALLELISM=False\n",
    "wsse = calculate_WSS(X, 10)\n",
    "\n",
    "wss_results = pd.DataFrame({'k':list(range(4,11)), 'wss' : wsse})\n",
    "\n",
    "ax1 = wss_results.plot.scatter(x='k',\n",
    "                      y='wss',\n",
    "                      c='DarkBlue')\n",
    "\n",
    "plt.plot(wss_results.k, wss_results.wss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Method')\n",
    "plt.show(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "\n",
    "def clustering_question(metadata,NUM_CLUSTERS):\n",
    "\n",
    "    sentences = metadata['doc_name']\n",
    "\n",
    "    X = np.array(metadata['vectors'].tolist())\n",
    "\n",
    "    kclusterer = KMeansClusterer(\n",
    "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
    "        repeats=25,avoid_empty_clusters=True)\n",
    "\n",
    "    assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "    metadata['cluster'] = pd.Series(assigned_clusters, index=metadata.index)\n",
    "    metadata['centroid'] = metadata['cluster'].apply(lambda x: kclusterer.means()[x])\n",
    "\n",
    "    return metadata, assigned_clusters\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil_avg = []\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "for k in range_n_clusters:\n",
    " kmeans = KMeans(n_clusters = k).fit(X)\n",
    " labels = kmeans.labels_\n",
    " sil_avg.append(silhouette_score(X, labels, metric = 'euclidean'))\n",
    "\n",
    "\n",
    "plt.plot(range_n_clusters,sil_avg,'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.title('Silhouette analysis For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "#Using the above Silhouette analysis, we can choose K’s optimal value as 3, 6, 8 \n",
    "#because the average silhouette score is higher and indicates that the \n",
    "#data points are optimally positioned.\n",
    "\n",
    "# Reduce the dimensionality of the embedding to 10 while keeping the size of the local neighborhood to 15\n",
    "\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15,\n",
    "                           n_components=5,\n",
    "                           metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "standard_embedding = umap.UMAP(random_state=42).fit_transform(list(metadata.vectors))\n",
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], c=mnist.target.astype(int), s=0.1, cmap='Spectral');\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
